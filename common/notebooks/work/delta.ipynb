{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57faf7d-2c74-432b-be96-482b38356736",
   "metadata": {},
   "source": [
    "## Set Up Spark Session for Delta Lake on MinIO\n",
    "\n",
    "This cell configures a SparkSession to work with **Delta Lake** using files stored on **MinIO** (S3-compatible storage).  \n",
    "The setup includes:\n",
    "\n",
    "- **Delta Lake support** via `delta-spark` package  \n",
    "- **S3A configuration** to connect to MinIO with access keys  \n",
    "- **Hadoop AWS** and **AWS SDK bundle** for S3 integration\n",
    "\n",
    "The `.config(...)` options enable Delta SQL features, register the Delta catalog, and tell Spark how to connect to MinIO using the S3A protocol.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe3c3a9-2052-4e46-95b5-92a808fb910b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "packages = \",\".join([\n",
    "    \"io.delta:delta-spark_2.12:3.2.0\",       \n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "    \"com.amazonaws:aws-java-sdk-bundle:1.12.530\",\n",
    "])\n",
    "\n",
    "# Create SparkSession with Delta and MinIO support\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"Delta Lakehouse on MinIO\")\n",
    "      .config(\"spark.jars.packages\", packages)\n",
    "      # Enable Delta Lake SQL support\n",
    "      .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "      .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "      # Configure S3A (MinIO) access\n",
    "      .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "      .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") # Enable path-style access (important for MinIO)\n",
    "      .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") # Disable SSL for local setup\n",
    "      .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "      .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "      .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "print(\"Spark:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f22134-9db8-49ee-bb24-46ee95e68913",
   "metadata": {},
   "source": [
    "## Create Delta Table from Parquet Files\n",
    "\n",
    "This step creates a new Delta Lake table `nyc.taxis_delta` in the `nyc` database.\n",
    "\n",
    "- The table is created using the `delta` format.\n",
    "- Data is loaded from two Parquet files stored in MinIO.\n",
    "- The table is saved to the Delta-compatible path: `s3a://my-bucket/delta-lakehouse/nyc/taxis`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da331652-84d9-4516-8d91-a0ed6b9e7aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS nyc\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nyc.taxis_delta\n",
    "USING delta\n",
    "LOCATION 's3a://my-bucket/delta-lakehouse/nyc/taxis'\n",
    "AS\n",
    "SELECT * FROM parquet.`s3a://my-bucket/raw-files/yellow_tripdata_2025-05.parquet`\n",
    "UNION ALL\n",
    "SELECT * FROM parquet.`s3a://my-bucket/raw-files/yellow_tripdata_2025-06.parquet`\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b50ef-c9f7-443a-bbe9-201342f7c844",
   "metadata": {},
   "source": [
    "## Verify Row Count in Delta Table\n",
    "\n",
    "This query checks how many rows were successfully loaded into the `nyc.taxis_delta` table.  \n",
    "It confirms that data from both Parquet files was ingested correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e103ce-dcaf-4e59-8d49-890420814e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 8914805|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM nyc.taxis_delta\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464dbdb-7d03-49d6-aee5-f9a47fa49c57",
   "metadata": {},
   "source": [
    "## Inspect Delta Table Metadata and History\n",
    "\n",
    "The first query (`DESCRIBE DETAIL`) shows metadata about the `nyc.taxis_delta` table, such as location, format, size, schema, and creation time.\n",
    "\n",
    "The second query (`DESCRIBE HISTORY`) displays the table's commit history — including operations like `CREATE`, `WRITE`, or `MERGE`, along with timestamps and user information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87d77828-4a8e-448b-8a64-40976ce01766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+--------------+\n",
      "|format|                  id|                name|description|            location|           createdAt|       lastModified|partitionColumns|clusteringColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion| tableFeatures|\n",
      "+------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+--------------+\n",
      "| delta|f7e66181-03b1-4bd...|spark_catalog.nyc...|       NULL|s3a://my-bucket/d...|2025-08-10 16:18:...|2025-08-10 16:18:26|              []|               []|      10|  183028768|        {}|               3|               7|[timestampNtz]|\n",
      "+------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+--------------+\n",
      "\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|userId|userName|           operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      0|2025-08-10 16:18:26|  NULL|    NULL|CREATE TABLE AS S...|{partitionBy -> [...|NULL|    NULL|     NULL|       NULL|  Serializable|         true|{numFiles -> 11, ...|        NULL|Apache-Spark/3.5....|\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "DESCRIBE DETAIL nyc.taxis_delta;\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\" \n",
    "DESCRIBE HISTORY nyc.taxis_delta;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74872156-7fec-4396-ac01-08756f1142f7",
   "metadata": {},
   "source": [
    "## Simulate New Batch Ingestion and Track Changes\n",
    "\n",
    "A small random sample (~10%) from the June Parquet file is inserted into the `nyc.taxis_delta` table to simulate a new batch of data.\n",
    "\n",
    "After the insert, `DESCRIBE HISTORY` is used again to verify that a new commit was recorded.  \n",
    "This allows tracking how and when the table was updated — a core feature of Delta Lake's transaction log.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79fe63b-1b0a-4f59-a13a-be5094d97b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|userId|userName|           operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      1|2025-08-10 16:18:29|  NULL|    NULL|               WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          0|  Serializable|         true|{numFiles -> 6, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      0|2025-08-10 16:18:26|  NULL|    NULL|CREATE TABLE AS S...|{partitionBy -> [...|NULL|    NULL|     NULL|       NULL|  Serializable|         true|{numFiles -> 11, ...|        NULL|Apache-Spark/3.5....|\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"\"\"INSERT INTO nyc.taxis_delta\n",
    "SELECT * FROM parquet.`s3a://my-bucket/raw-files/yellow_tripdata_2025-06.parquet`\n",
    "WHERE rand() < 0.10;\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"DESCRIBE HISTORY nyc.taxis_delta;\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4edff-27b2-45d8-8005-a4af4f3b2849",
   "metadata": {},
   "source": [
    "## Query Delta Table at Specific Versions\n",
    "\n",
    "These queries read the `nyc.taxis_delta` table as it existed at earlier points in time using Delta Lake’s **time travel** feature:\n",
    "\n",
    "- `VERSION AS OF 0` returns the original state of the table (after the initial load).\n",
    "- `VERSION AS OF 1` includes the additional rows from the simulated batch insert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b663638-db62-4ed3-9630-1c81dcb0cedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 8914805|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 9346736|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "SELECT COUNT(*) \n",
    "FROM delta.`s3a://my-bucket/delta-lakehouse/nyc/taxis` VERSION AS OF 0;\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\" \n",
    "SELECT COUNT(*) \n",
    "FROM delta.`s3a://my-bucket/delta-lakehouse/nyc/taxis` VERSION AS OF 1;\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80094696-3d90-4074-be42-85d481ce9b21",
   "metadata": {},
   "source": [
    "## Restore Delta Table to an Earlier Version\n",
    "\n",
    "This command rolls back the `nyc.taxis_delta` table to version 0 — effectively undoing the last insert.\n",
    "The `DESCRIBE HISTORY` query then shows a new entry for the `RESTORE` operation, confirming that the rollback was recorded in the Delta transaction log.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b79c318-b82a-49d4-aa24-014afd06899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|userId|userName|           operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      2|2025-08-10 16:18:33|  NULL|    NULL|             RESTORE|{version -> 0, ti...|NULL|    NULL|     NULL|          1|  Serializable|        false|{numRestoredFiles...|        NULL|Apache-Spark/3.5....|\n",
      "|      1|2025-08-10 16:18:29|  NULL|    NULL|               WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          0|  Serializable|         true|{numFiles -> 6, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      0|2025-08-10 16:18:26|  NULL|    NULL|CREATE TABLE AS S...|{partitionBy -> [...|NULL|    NULL|     NULL|       NULL|  Serializable|         true|{numFiles -> 11, ...|        NULL|Apache-Spark/3.5....|\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "RESTORE TABLE nyc.taxis_delta TO VERSION AS OF 0;\"\"\")\n",
    "\n",
    "spark.sql(\"\"\" \n",
    "DESCRIBE HISTORY nyc.taxis_delta;\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decfe775-e245-4147-a881-57ea5e5e8882",
   "metadata": {},
   "source": [
    "## Delta Lake Change Data Feed (CDF) Demo\n",
    "\n",
    "This sequence shows how to enable and use CDF on `nyc.taxis_delta`:\n",
    "\n",
    "1. **Enable CDF**  \n",
    "   Turn on `delta.enableChangeDataFeed = true` so future commits record row-level changes.\n",
    "\n",
    "2. **Mark the starting version**  \n",
    "   Read `DESCRIBE HISTORY` and capture the current table version before making changes.\n",
    "\n",
    "3. **Make controlled changes**  \n",
    "   - `INSERT` a tiny, deterministic slice (one day, small sample).  \n",
    "   - `UPDATE` a subset (e.g., set `tip_amount = 0` for a day/vendor filter).  \n",
    "   - `DELETE` a few rows (e.g., very short trips that day).\n",
    "\n",
    "4. **Read only the changes**  \n",
    "   Load the table with:\n",
    "   - `readChangeFeed = true`  \n",
    "   - `startingVersion = <captured version>`  \n",
    "   - `endingVersion = latest`  \n",
    "   This returns only changed rows between those versions.\n",
    "\n",
    "5. **Inspect change types**  \n",
    "   CDF adds metadata columns:\n",
    "   - `_change_type`: `insert`, `update_preimage`, `update_postimage`, `delete`  \n",
    "   - `_commit_version`, `_commit_timestamp`  \n",
    "   Select relevant business columns plus these fields to see exactly what changed.  \n",
    "   (Optional) `GROUP BY _change_type` to summarize counts.\n",
    "\n",
    "Notes:\n",
    "- CDF starts recording **after** it’s enabled; earlier versions won’t have change rows.\n",
    "- If filters affect few records, you may only see `insert` rows (or small counts) in CDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8b272f6-d33c-429f-910f-00aa34bc04a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+----------------+--------+--------------------+----------+------------+\n",
      "|_commit_version|_commit_timestamp     |_change_type    |VendorID|tpep_pickup_datetime|tip_amount|total_amount|\n",
      "+---------------+----------------------+----------------+--------+--------------------+----------+------------+\n",
      "|5              |2025-08-10 16:18:39.37|update_preimage |2       |2025-06-15 00:02:13 |3.29      |19.74       |\n",
      "|5              |2025-08-10 16:18:39.37|update_postimage|2       |2025-06-15 00:02:13 |0.0       |19.74       |\n",
      "|5              |2025-08-10 16:18:39.37|update_preimage |2       |2025-06-15 00:44:11 |3.45      |14.95       |\n",
      "|5              |2025-08-10 16:18:39.37|update_postimage|2       |2025-06-15 00:44:11 |0.0       |14.95       |\n",
      "|5              |2025-08-10 16:18:39.37|update_preimage |1       |2025-06-15 00:16:04 |0.0       |68.85       |\n",
      "|5              |2025-08-10 16:18:39.37|update_postimage|1       |2025-06-15 00:16:04 |0.0       |68.85       |\n",
      "|5              |2025-08-10 16:18:39.37|update_preimage |2       |2025-06-15 00:06:30 |0.0       |-19.2       |\n",
      "|5              |2025-08-10 16:18:39.37|update_postimage|2       |2025-06-15 00:06:30 |0.0       |-19.2       |\n",
      "|5              |2025-08-10 16:18:39.37|update_preimage |2       |2025-06-15 00:06:30 |0.0       |19.2        |\n",
      "|5              |2025-08-10 16:18:39.37|update_postimage|2       |2025-06-15 00:06:30 |0.0       |19.2        |\n",
      "|5              |2025-08-10 16:18:39.37|update_preimage |2       |2025-06-15 00:09:34 |5.0       |61.8        |\n",
      "|5              |2025-08-10 16:18:39.37|update_postimage|2       |2025-06-15 00:09:34 |0.0       |61.8        |\n",
      "|5              |2025-08-10 16:18:39.37|update_preimage |1       |2025-06-15 00:48:52 |5.25      |31.5        |\n",
      "|5              |2025-08-10 16:18:39.37|update_postimage|1       |2025-06-15 00:48:52 |0.0       |31.5        |\n",
      "|5              |2025-08-10 16:18:39.37|update_preimage |2       |2025-06-15 00:43:39 |0.0       |11.5        |\n",
      "|5              |2025-08-10 16:18:39.37|update_postimage|2       |2025-06-15 00:43:39 |0.0       |11.5        |\n",
      "|5              |2025-08-10 16:18:39.37|update_preimage |2       |2025-06-15 00:24:50 |3.54      |15.34       |\n",
      "|5              |2025-08-10 16:18:39.37|update_postimage|2       |2025-06-15 00:24:50 |0.0       |15.34       |\n",
      "|5              |2025-08-10 16:18:39.37|update_preimage |2       |2025-06-15 00:59:36 |4.93      |21.38       |\n",
      "|5              |2025-08-10 16:18:39.37|update_postimage|2       |2025-06-15 00:59:36 |0.0       |21.38       |\n",
      "+---------------+----------------------+----------------+--------+--------------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1) Enable CDF (applies to future commits)\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE nyc.taxis_delta\n",
    "SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\")\n",
    "\n",
    "# 2) Capture the current version BEFORE changes\n",
    "start_version = (spark.sql(\"DESCRIBE HISTORY nyc.taxis_delta\")\n",
    "                   .selectExpr(\"max(version) as v\").collect()[0][0])\n",
    "\n",
    "# 3) Make some changes: INSERT + UPDATE + DELETE\n",
    "# Insert a small, deterministic slice (one day, tiny sample)\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO nyc.taxis_delta\n",
    "SELECT *\n",
    "FROM parquet.`s3a://my-bucket/raw-files/yellow_tripdata_2025-06.parquet`\n",
    "WHERE tpep_pickup_datetime >= '2025-06-15'\n",
    "  AND tpep_pickup_datetime <  '2025-06-16'\n",
    "  AND rand() < 0.02\n",
    "\"\"\")\n",
    "\n",
    "# Update a small subset (same day, common vendors)\n",
    "spark.sql(\"\"\"\n",
    "UPDATE nyc.taxis_delta\n",
    "SET tip_amount = 0.0\n",
    "WHERE date(tpep_pickup_datetime) = '2025-06-15' AND VendorID IN (1,2)\n",
    "\"\"\")\n",
    "\n",
    "# Delete a tiny subset from that day (very short trips)\n",
    "spark.sql(\"\"\"\n",
    "DELETE FROM nyc.taxis_delta\n",
    "WHERE date(tpep_pickup_datetime) = '2025-06-15' AND trip_distance < 0.1\n",
    "\"\"\")\n",
    "\n",
    "# 4) Read just the changes between start_version -> latest\n",
    "end_version = (spark.sql(\"DESCRIBE HISTORY nyc.taxis_delta\")\n",
    "                 .selectExpr(\"max(version) as v\").collect()[0][0])\n",
    "\n",
    "cdf = (spark.read.format(\"delta\")\n",
    "       .option(\"readChangeFeed\", \"true\")\n",
    "       .option(\"startingVersion\", start_version)\n",
    "       .option(\"endingVersion\", end_version)\n",
    "       .table(\"nyc.taxis_delta\"))\n",
    "\n",
    "# 5) Inspect change types\n",
    "cdf.select(\"_commit_version\", \"_commit_timestamp\", \"_change_type\",\n",
    "           \"VendorID\", \"tpep_pickup_datetime\", \"tip_amount\", \"total_amount\") \\\n",
    "   .show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138f5120-ebc0-41f3-9401-0c9a7b3d4f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|    _change_type| count|\n",
      "+----------------+------+\n",
      "| update_preimage|125545|\n",
      "|update_postimage|125545|\n",
      "|          delete|  2088|\n",
      "|          insert|  2493|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cdf.groupBy(\"_change_type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e610b206-0bed-4e17-85aa-449ac795431c",
   "metadata": {},
   "source": [
    "# Add and Populate a New Column in Delta Table\n",
    "\n",
    "This step adds a new column `fare_bucket` to the `nyc.taxis_delta` table and fills it with values based on `total_amount`:\n",
    "\n",
    "- `'low'` for fares under 10  \n",
    "- `'mid'` for fares between 10 and 30  \n",
    "- `'high'` for fares 30 and above\n",
    "\n",
    "The final query groups the data by `fare_bucket` to show the distribution of fare ranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ab543c6-73c4-4d67-8c7e-5c29d00e603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|fare_bucket|count(1)|\n",
      "+-----------+--------+\n",
      "|        low|  765108|\n",
      "|        mid| 5744522|\n",
      "|       high| 2405580|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Disabling CDF because of Java OOM\n",
    "spark.sql(\"\"\"ALTER TABLE nyc.taxis_delta UNSET TBLPROPERTIES (delta.enableChangeDataFeed);\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"ALTER TABLE nyc.taxis_delta ADD COLUMNS (fare_bucket STRING);\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"UPDATE nyc.taxis_delta\n",
    "SET fare_bucket = CASE\n",
    "  WHEN total_amount < 10 THEN 'low'\n",
    "  WHEN total_amount < 30 THEN 'mid'\n",
    "  ELSE 'high'\n",
    "END\n",
    "WHERE total_amount IS NOT NULL;\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT fare_bucket, COUNT(*) \n",
    "FROM nyc.taxis_delta \n",
    "GROUP BY fare_bucket;\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de3474e-7599-4b03-b6d5-1a03d36ca5d4",
   "metadata": {},
   "source": [
    "## Partition Evolution in Delta Lake\n",
    "\n",
    "Delta Lake does not support partition evolution.  \n",
    "Once a table’s partitioning is defined at creation, it cannot be changed without recreating the table and reloading the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7dd770-572f-4037-ac07-0f7dae83fddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
