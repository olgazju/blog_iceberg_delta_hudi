{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57faf7d-2c74-432b-be96-482b38356736",
   "metadata": {},
   "source": [
    "## Set Up Spark Session for Delta Lake on MinIO\n",
    "\n",
    "This cell configures a SparkSession to work with **Delta Lake** using files stored on **MinIO** (S3-compatible storage).  \n",
    "The setup includes:\n",
    "\n",
    "- **Delta Lake support** via `delta-spark` package  \n",
    "- **S3A configuration** to connect to MinIO with access keys  \n",
    "- **Hadoop AWS** and **AWS SDK bundle** for S3 integration\n",
    "\n",
    "The `.config(...)` options enable Delta SQL features, register the Delta catalog, and tell Spark how to connect to MinIO using the S3A protocol.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe3c3a9-2052-4e46-95b5-92a808fb910b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "packages = \",\".join([\n",
    "    \"io.delta:delta-spark_2.12:3.2.0\",       \n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "    \"com.amazonaws:aws-java-sdk-bundle:1.12.530\",\n",
    "])\n",
    "\n",
    "# Create SparkSession with Delta and MinIO support\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"Delta Lakehouse on MinIO\")\n",
    "      .config(\"spark.jars.packages\", packages)\n",
    "      # Enable Delta Lake SQL support\n",
    "      .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "      .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "      # Configure S3A (MinIO) access\n",
    "      .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "      .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") # Enable path-style access (important for MinIO)\n",
    "      .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") # Disable SSL for local setup\n",
    "      .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "      .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "      .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "print(\"Spark:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f22134-9db8-49ee-bb24-46ee95e68913",
   "metadata": {},
   "source": [
    "## Create Delta Table from Parquet Files\n",
    "\n",
    "This step creates a new Delta Lake table `nyc.taxis_delta` in the `nyc` database.\n",
    "\n",
    "- The table is created using the `delta` format.\n",
    "- Data is loaded from two Parquet files stored in MinIO.\n",
    "- The table is saved to the Delta-compatible path: `s3a://my-bucket/delta-lakehouse/nyc/taxis`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da331652-84d9-4516-8d91-a0ed6b9e7aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS nyc\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nyc.taxis_delta\n",
    "USING delta\n",
    "LOCATION 's3a://my-bucket/delta-lakehouse/nyc/taxis'\n",
    "AS\n",
    "SELECT * FROM parquet.`s3a://my-bucket/raw-files/yellow_tripdata_2025-05.parquet`\n",
    "UNION ALL\n",
    "SELECT * FROM parquet.`s3a://my-bucket/raw-files/yellow_tripdata_2025-06.parquet`\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b50ef-c9f7-443a-bbe9-201342f7c844",
   "metadata": {},
   "source": [
    "## Verify Row Count in Delta Table\n",
    "\n",
    "This query checks how many rows were successfully loaded into the `nyc.taxis_delta` table.  \n",
    "It confirms that data from both Parquet files was ingested correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42e103ce-dcaf-4e59-8d49-890420814e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 8914805|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM nyc.taxis_delta\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464dbdb-7d03-49d6-aee5-f9a47fa49c57",
   "metadata": {},
   "source": [
    "## Inspect Delta Table Metadata and History\n",
    "\n",
    "The first query (`DESCRIBE DETAIL`) shows metadata about the `nyc.taxis_delta` table, such as location, format, size, schema, and creation time.\n",
    "\n",
    "The second query (`DESCRIBE HISTORY`) displays the table's commit history — including operations like `CREATE`, `WRITE`, or `MERGE`, along with timestamps and user information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d77828-4a8e-448b-8a64-40976ce01766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+--------------+\n",
      "|format|                  id|                name|description|            location|           createdAt|       lastModified|partitionColumns|clusteringColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion| tableFeatures|\n",
      "+------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+--------------+\n",
      "| delta|1154326b-475a-4ed...|spark_catalog.nyc...|       NULL|s3a://my-bucket/d...|2025-08-09 16:48:...|2025-08-09 16:48:21|              []|               []|      10|  183028768|        {}|               3|               7|[timestampNtz]|\n",
      "+------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+--------------+\n",
      "\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|userId|userName|           operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      0|2025-08-09 16:48:21|  NULL|    NULL|CREATE TABLE AS S...|{partitionBy -> [...|NULL|    NULL|     NULL|       NULL|  Serializable|         true|{numFiles -> 11, ...|        NULL|Apache-Spark/3.5....|\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "DESCRIBE DETAIL nyc.taxis_delta;\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\" \n",
    "DESCRIBE HISTORY nyc.taxis_delta;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74872156-7fec-4396-ac01-08756f1142f7",
   "metadata": {},
   "source": [
    "## Simulate New Batch Ingestion and Track Changes\n",
    "\n",
    "A small random sample (~10%) from the June Parquet file is inserted into the `nyc.taxis_delta` table to simulate a new batch of data.\n",
    "\n",
    "After the insert, `DESCRIBE HISTORY` is used again to verify that a new commit was recorded.  \n",
    "This allows tracking how and when the table was updated — a core feature of Delta Lake's transaction log.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e79fe63b-1b0a-4f59-a13a-be5094d97b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|userId|userName|           operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      1|2025-08-09 16:49:45|  NULL|    NULL|               WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          0|  Serializable|         true|{numFiles -> 6, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      0|2025-08-09 16:48:21|  NULL|    NULL|CREATE TABLE AS S...|{partitionBy -> [...|NULL|    NULL|     NULL|       NULL|  Serializable|         true|{numFiles -> 11, ...|        NULL|Apache-Spark/3.5....|\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"\"\"INSERT INTO nyc.taxis_delta\n",
    "SELECT * FROM parquet.`s3a://my-bucket/raw-files/yellow_tripdata_2025-06.parquet`\n",
    "WHERE rand() < 0.10;\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"DESCRIBE HISTORY nyc.taxis_delta;\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4edff-27b2-45d8-8005-a4af4f3b2849",
   "metadata": {},
   "source": [
    "## Query Delta Table at Specific Versions\n",
    "\n",
    "These queries read the `nyc.taxis_delta` table as it existed at earlier points in time using Delta Lake’s **time travel** feature:\n",
    "\n",
    "- `VERSION AS OF 0` returns the original state of the table (after the initial load).\n",
    "- `VERSION AS OF 1` includes the additional rows from the simulated batch insert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b663638-db62-4ed3-9630-1c81dcb0cedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 8914805|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 9346535|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "SELECT COUNT(*) \n",
    "FROM delta.`s3a://my-bucket/delta-lakehouse/nyc/taxis` VERSION AS OF 0;\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\" \n",
    "SELECT COUNT(*) \n",
    "FROM delta.`s3a://my-bucket/delta-lakehouse/nyc/taxis` VERSION AS OF 1;\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80094696-3d90-4074-be42-85d481ce9b21",
   "metadata": {},
   "source": [
    "## Restore Delta Table to an Earlier Version\n",
    "\n",
    "This command rolls back the `nyc.taxis_delta` table to version 0 — effectively undoing the last insert.\n",
    "The `DESCRIBE HISTORY` query then shows a new entry for the `RESTORE` operation, confirming that the rollback was recorded in the Delta transaction log.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b79c318-b82a-49d4-aa24-014afd06899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|userId|userName|           operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      2|2025-08-09 16:50:48|  NULL|    NULL|             RESTORE|{version -> 0, ti...|NULL|    NULL|     NULL|          1|  Serializable|        false|{numRestoredFiles...|        NULL|Apache-Spark/3.5....|\n",
      "|      1|2025-08-09 16:49:45|  NULL|    NULL|               WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          0|  Serializable|         true|{numFiles -> 6, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      0|2025-08-09 16:48:21|  NULL|    NULL|CREATE TABLE AS S...|{partitionBy -> [...|NULL|    NULL|     NULL|       NULL|  Serializable|         true|{numFiles -> 11, ...|        NULL|Apache-Spark/3.5....|\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "RESTORE TABLE nyc.taxis_delta TO VERSION AS OF 0;\"\"\")\n",
    "\n",
    "spark.sql(\"\"\" \n",
    "DESCRIBE HISTORY nyc.taxis_delta;\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e610b206-0bed-4e17-85aa-449ac795431c",
   "metadata": {},
   "source": [
    "# Add and Populate a New Column in Delta Table\n",
    "\n",
    "This step adds a new column `fare_bucket` to the `nyc.taxis_delta` table and fills it with values based on `total_amount`:\n",
    "\n",
    "- `'low'` for fares under 10  \n",
    "- `'mid'` for fares between 10 and 30  \n",
    "- `'high'` for fares 30 and above\n",
    "\n",
    "The final query groups the data by `fare_bucket` to show the distribution of fare ranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ab543c6-73c4-4d67-8c7e-5c29d00e603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|fare_bucket|count(1)|\n",
      "+-----------+--------+\n",
      "|        low|  765654|\n",
      "|        mid| 5743528|\n",
      "|       high| 2405623|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"ALTER TABLE nyc.taxis_delta ADD COLUMNS (fare_bucket STRING);\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"UPDATE nyc.taxis_delta\n",
    "SET fare_bucket = CASE\n",
    "  WHEN total_amount < 10 THEN 'low'\n",
    "  WHEN total_amount < 30 THEN 'mid'\n",
    "  ELSE 'high'\n",
    "END\n",
    "WHERE total_amount IS NOT NULL;\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT fare_bucket, COUNT(*) \n",
    "FROM nyc.taxis_delta \n",
    "GROUP BY fare_bucket;\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de3474e-7599-4b03-b6d5-1a03d36ca5d4",
   "metadata": {},
   "source": [
    "## Partition Evolution in Delta Lake\n",
    "\n",
    "Delta Lake does not support partition evolution.  \n",
    "Once a table’s partitioning is defined at creation, it cannot be changed without recreating the table and reloading the data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
