{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e9f783-fa22-4b58-adcd-f2420765d505",
   "metadata": {},
   "source": [
    "## Setup Spark with Iceberg and MinIO\n",
    "This configures a Spark session with Iceberg support and sets up the lake catalog to use MinIO as the warehouse (s3a://my-bucket/iceberg-lakehouse).\n",
    "We're using HadoopCatalog and enabling S3A access with local MinIO credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce008fea-3e04-46ff-83d7-aea51be52970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "packages = \",\".join([\n",
    "    \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.2\", # Iceberg runtime for Spark 3.5\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\",  # Hadoop support for AWS S3 (S3A)\n",
    "    \"com.amazonaws:aws-java-sdk-bundle:1.12.530\",  # AWS SDK bundle (used under the hood)\n",
    "])\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"Iceberg Lakehouse on MinIO\")\n",
    "      .config(\"spark.jars.packages\", packages)\n",
    "      .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") # Enable Iceberg SQL support\n",
    "      .config(\"spark.sql.defaultCatalog\", \"lake\") # Use 'lake' as the default catalog for Spark SQL\n",
    "\n",
    "      # Define the 'lake' Iceberg catalog using HadoopCatalog backed by MinIO \n",
    "      .config(\"spark.sql.catalog.lake\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "      .config(\"spark.sql.catalog.lake.type\", \"hadoop\")\n",
    "      .config(\"spark.sql.catalog.lake.warehouse\", \"s3a://my-bucket/iceberg-lakehouse\")\n",
    "\n",
    "      # MinIO (S3-compatible) connection configs\n",
    "      .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "      .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") # Enable path-style access (important for MinIO)\n",
    "      .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") # Disable SSL for local setup\n",
    "      .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "      .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "      .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "\n",
    "      .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c46c2-28e2-45ad-95fc-be9f3fea2749",
   "metadata": {},
   "source": [
    "## Create Iceberg Table from Existing Parquet Files\n",
    "\n",
    "We switch to the lake catalog, create the nyc namespace and define a new Iceberg table nyc.taxis by loading two raw Parquet files from MinIO.\n",
    "This creates a managed Iceberg table that supports versioning, time travel and schema evolution from the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621c011d-78cc-4c31-8467-f753ab565d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.setCurrentCatalog(\"lake\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nyc\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nyc.taxis\n",
    "USING iceberg\n",
    "AS\n",
    "SELECT * FROM parquet.`s3a://my-bucket/raw-files/yellow_tripdata_2025-05.parquet`\n",
    "UNION ALL\n",
    "SELECT * FROM parquet.`s3a://my-bucket/raw-files/yellow_tripdata_2025-06.parquet`\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a76594b-e554-419a-86fe-da640e95581e",
   "metadata": {},
   "source": [
    "## Preview a few rows from nyc.taxis\n",
    "Run a quick Spark SQL sanity check to verify the table is readable and the expected columns are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa182c6-37d7-493c-8deb-0bf7336a602a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+--------+---------------+------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|VendorID|passenger_count|total_amount|\n",
      "+--------------------+---------------------+--------+---------------+------------+\n",
      "| 2025-05-01 00:07:06|  2025-05-01 00:24:15|       1|              1|        29.0|\n",
      "| 2025-05-01 00:07:44|  2025-05-01 00:14:27|       2|              1|       18.65|\n",
      "| 2025-05-01 00:15:56|  2025-05-01 00:23:53|       2|              1|       15.75|\n",
      "| 2025-05-01 00:00:09|  2025-05-01 00:25:29|       2|              1|       71.94|\n",
      "| 2025-05-01 00:45:07|  2025-05-01 00:52:45|       2|              1|       17.25|\n",
      "+--------------------+---------------------+--------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT tpep_pickup_datetime, tpep_dropoff_datetime, VendorID, passenger_count, total_amount  FROM nyc.taxis limit 5;\n",
    "\"\"\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16792aca-65a6-41ff-b306-df205c3f4e20",
   "metadata": {},
   "source": [
    "## View Table History for nyc.taxis\n",
    "Iceberg keeps track of table version history, including metadata about schema changes, snapshot lineage, and when each version became active. In snapshot_id you can see your first snapshot id and file for it you can find s3://my-bucket/iceberg-lakehouse/nyc/taxis/metadata/snap-{snapshot_id}-**.avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "918cae61-6bc4-4845-9b16-11b6addd16d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+---------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id|is_current_ancestor|\n",
      "+-----------------------+-------------------+---------+-------------------+\n",
      "|2025-08-11 16:00:43.386|8062238580457074585|NULL     |true               |\n",
      "+-----------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM nyc.taxis.history ORDER BY made_current_at DESC;\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077e831-4342-468d-afdf-e61a1612f972",
   "metadata": {},
   "source": [
    "## Check Data Files in nyc.taxis\n",
    "This shows the list of files in the Iceberg table, including:\n",
    "\n",
    "file_path: where each file is stored\n",
    "record_count: how many rows in each file\n",
    "file_size_in_bytes: file size\n",
    "content: type of data (e.g. data, delete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "775f2871-8f08-4364-a7e4-b512e4b8b4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|content|file_path                                                                                                     |record_count|file_size_in_bytes|\n",
      "+-------+--------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|0      |s3a://my-bucket/iceberg-lakehouse/nyc/taxis/data/00001-3-22c9cd14-c318-4214-b7ef-3a349bd60b7f-0-00001.parquet |1048576     |16886663          |\n",
      "|0      |s3a://my-bucket/iceberg-lakehouse/nyc/taxis/data/00003-5-22c9cd14-c318-4214-b7ef-3a349bd60b7f-0-00001.parquet |1048576     |16960598          |\n",
      "|0      |s3a://my-bucket/iceberg-lakehouse/nyc/taxis/data/00006-8-22c9cd14-c318-4214-b7ef-3a349bd60b7f-0-00001.parquet |1048576     |16872467          |\n",
      "|0      |s3a://my-bucket/iceberg-lakehouse/nyc/taxis/data/00009-11-22c9cd14-c318-4214-b7ef-3a349bd60b7f-0-00001.parquet|1048576     |16697644          |\n",
      "|0      |s3a://my-bucket/iceberg-lakehouse/nyc/taxis/data/00010-12-22c9cd14-c318-4214-b7ef-3a349bd60b7f-0-00001.parquet|397541      |6495049           |\n",
      "|0      |s3a://my-bucket/iceberg-lakehouse/nyc/taxis/data/00013-15-22c9cd14-c318-4214-b7ef-3a349bd60b7f-0-00001.parquet|1048576     |17062882          |\n",
      "|0      |s3a://my-bucket/iceberg-lakehouse/nyc/taxis/data/00016-18-22c9cd14-c318-4214-b7ef-3a349bd60b7f-0-00001.parquet|1048576     |17073641          |\n",
      "|0      |s3a://my-bucket/iceberg-lakehouse/nyc/taxis/data/00018-20-22c9cd14-c318-4214-b7ef-3a349bd60b7f-0-00001.parquet|1048576     |17094953          |\n",
      "|0      |s3a://my-bucket/iceberg-lakehouse/nyc/taxis/data/00021-23-22c9cd14-c318-4214-b7ef-3a349bd60b7f-0-00001.parquet|1048576     |15660782          |\n",
      "|0      |s3a://my-bucket/iceberg-lakehouse/nyc/taxis/data/00023-25-22c9cd14-c318-4214-b7ef-3a349bd60b7f-0-00001.parquet|128656      |2258654           |\n",
      "+-------+--------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT content, file_path, record_count, file_size_in_bytes\n",
    "FROM nyc.taxis.files;\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d60285-0271-48d6-82b5-fb50445cba6a",
   "metadata": {},
   "source": [
    "## Simulate a New Batch Ingestion\n",
    "This cell simulates a new batch arriving by inserting a random 10% sample of rows from the June dataset into the nyc.taxis table.\n",
    "After the insert, the history table is queried again to show that a new snapshot has been added.\n",
    "\n",
    "This demonstrates how Iceberg creates a new table version with each write, enabling time travel and version tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d09d9979-4e08-47fa-804d-da2a0b2ff78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2025-08-11 16:00:45.126|4274936221537877971|8062238580457074585|true               |\n",
      "|2025-08-11 16:00:43.386|8062238580457074585|NULL               |true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO nyc.taxis\n",
    "SELECT * FROM parquet.`s3a://my-bucket/raw-files/yellow_tripdata_2025-06.parquet` WHERE rand() < 0.10;\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM nyc.taxis.history ORDER BY made_current_at DESC;\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf9939c-1372-42ac-b3ef-fae4c4470262",
   "metadata": {},
   "source": [
    "## Save Snapshot IDs for Time Travel\n",
    "Two snapshot IDs are copied from the nyc.taxis.history table to use in the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e2d80cb-0c62-4a53-9a10-636884df8cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_id_1='8062238580457074585'\n",
    "snapshot_id_2='4274936221537877971'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b2e91f-0eb5-4b3f-9e49-f39df06ea8e0",
   "metadata": {},
   "source": [
    "## Compare Row Counts Between Snapshots\n",
    "This runs two queries to count the number of rows in the nyc.taxis table at two different points in time — before and after the simulated batch ingest.\n",
    "\n",
    "The lower snapshot ID represents the original state of the table.\n",
    "The higher one reflects the updated state after inserting 10% more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01171428-2ae1-440f-bb1b-50e6db803628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|8914805 |\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|9347281 |\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT count(*) FROM nyc.taxis VERSION AS OF {snapshot_id_1} LIMIT 5;\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT count(*) FROM nyc.taxis VERSION AS OF {snapshot_id_2} LIMIT 5;\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d1011-6928-46e0-8422-609f35047b86",
   "metadata": {},
   "source": [
    "## Roll Back Table to an Earlier Snapshot\n",
    "\n",
    "This command reverts the nyc.taxis table back to a previous state using the older snapshot ID.\n",
    "It’s a simple way to undo changes and return the table to how it looked before the last ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a85ec6c-863f-4cef-bd2a-a644ff60352c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[previous_snapshot_id: bigint, current_snapshot_id: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\" \n",
    "CALL lake.system.rollback_to_snapshot('nyc.taxis', {snapshot_id_1})\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecee3e46-9779-4d78-a67f-cf060807aef1",
   "metadata": {},
   "source": [
    "## Verify the Rollback\n",
    "This query checks the nyc.taxis.history table again to confirm that the rollback took effect.\n",
    "A new snapshot should appear at the top, pointing back to the older snapshot ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c38fbb1-d385-45a0-8183-15061504f3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2025-08-11 16:01:07.283|8062238580457074585|NULL               |true               |\n",
      "|2025-08-11 16:00:45.126|4274936221537877971|8062238580457074585|false              |\n",
      "|2025-08-11 16:00:43.386|8062238580457074585|NULL               |true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM nyc.taxis.history ORDER BY made_current_at DESC;\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f38c4e-1996-49c6-807b-764f78486fb1",
   "metadata": {},
   "source": [
    "## Check Row Count After Rollback\n",
    "This query returns the current number of rows in the nyc.taxis table, after the rollback.\n",
    "It should match the count from the older snapshot (before the extra batch was inserted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d19a52ed-58f1-40e2-8576-b8b08d27cf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|8914805 |\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT count(*) FROM nyc.taxis  ;\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb46e7c6-7e5b-4798-9c5d-95ef476f5143",
   "metadata": {},
   "source": [
    "## Add and Populate a New Column\n",
    "This sequence adds a new column fare_bucket to the nyc.taxis table, classifying rides based on the total_amount.\n",
    "The column is added after total_amount, then populated with a basic categorization logic:\n",
    "\n",
    "'low' for fares under 10\n",
    "\n",
    "'mid' for fares between 10 and 30\n",
    "\n",
    "'high' for fares above 30\n",
    "\n",
    "Finally, the results are grouped by fare_bucket to show the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2274c435-a0dc-43c7-9c6d-f888cdda9869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|fare_bucket|count(1)|\n",
      "+-----------+--------+\n",
      "|low        |765654  |\n",
      "|mid        |5743528 |\n",
      "|high       |2405623 |\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "spark.sql(\"\"\"ALTER TABLE nyc.taxis ADD COLUMN fare_bucket STRING AFTER total_amount;\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"-- update a derived field to prove reads see the new column\n",
    "UPDATE nyc.taxis\n",
    "SET fare_bucket = CASE\n",
    "  WHEN total_amount < 10 THEN 'low'\n",
    "  WHEN total_amount < 30 THEN 'mid'\n",
    "  ELSE 'high'\n",
    "END\n",
    "WHERE total_amount IS NOT NULL;\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT fare_bucket, COUNT(*) FROM nyc.taxis GROUP BY fare_bucket;\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b027d44-211a-4930-86ae-ffe25f109e2a",
   "metadata": {},
   "source": [
    "## Evolve Iceberg Schema On Write with `accept-any-schema`\n",
    "\n",
    "In Iceberg, new columns can be added not only through `ALTER TABLE`, but also automatically during writes.\n",
    "\n",
    "To enable this behavior, we switch the table to flexible write mode:\n",
    "\n",
    "- Drop the `fare_bucket` column to simplify the schema before adding something new.\n",
    "- Set the table property `'write.spark.accept-any-schema' = true` to allow appending data with previously unseen columns.\n",
    "- Add a new derived column `tip_rate_pct` directly during ingestion — no schema pre-declaration needed.\n",
    "- Use `.option(\"mergeSchema\", \"true\")` to tell Iceberg to evolve the schema automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34ddc6ce-8b75-42cb-ae3a-9aafb3c04b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------+\n",
      "|            col_name|    data_type|comment|\n",
      "+--------------------+-------------+-------+\n",
      "|            VendorID|          int|   NULL|\n",
      "|tpep_pickup_datetime|timestamp_ntz|   NULL|\n",
      "|tpep_dropoff_date...|timestamp_ntz|   NULL|\n",
      "|     passenger_count|       bigint|   NULL|\n",
      "|       trip_distance|       double|   NULL|\n",
      "|          RatecodeID|       bigint|   NULL|\n",
      "|  store_and_fwd_flag|       string|   NULL|\n",
      "|        PULocationID|          int|   NULL|\n",
      "|        DOLocationID|          int|   NULL|\n",
      "|        payment_type|       bigint|   NULL|\n",
      "|         fare_amount|       double|   NULL|\n",
      "|               extra|       double|   NULL|\n",
      "|             mta_tax|       double|   NULL|\n",
      "|          tip_amount|       double|   NULL|\n",
      "|        tolls_amount|       double|   NULL|\n",
      "|improvement_surch...|       double|   NULL|\n",
      "|        total_amount|       double|   NULL|\n",
      "|congestion_surcharge|       double|   NULL|\n",
      "|         Airport_fee|       double|   NULL|\n",
      "|  cbd_congestion_fee|       double|   NULL|\n",
      "|        tip_rate_pct|       double|   NULL|\n",
      "+--------------------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from pyspark.sql.functions import col, when, lit, udf, round as sround\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "spark.sql(\"ALTER TABLE lake.nyc.taxis DROP COLUMN fare_bucket\")\n",
    "spark.sql(\"\"\"ALTER TABLE lake.nyc.taxis SET TBLPROPERTIES (\n",
    "  'write.spark.accept-any-schema'='true'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Load the raw parquet file\n",
    "df = spark.read.parquet(\"s3a://my-bucket/raw-files/yellow_tripdata_2025-06.parquet\")\n",
    "\n",
    "# Compute tip_rate_pct as a percentage of the total fare\n",
    "df_with_tip = (\n",
    "    df.withColumn(\n",
    "        \"tip_rate_pct\",\n",
    "        when(col(\"total_amount\").isNull(), lit(None).cast(\"double\"))\n",
    "        .when(col(\"total_amount\") <= 0, lit(None).cast(\"double\"))\n",
    "        .otherwise(sround(col(\"tip_amount\") / col(\"total_amount\") * 100.0, 2))\n",
    "    )\n",
    ")\n",
    "\n",
    "(df_with_tip.writeTo(\"lake.nyc.taxis\")\n",
    "  .option(\"mergeSchema\", \"true\")  # Enables schema evolution on write\n",
    "  .append())\n",
    "\n",
    "spark.sql(\"DESCRIBE TABLE lake.nyc.taxis;\").show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d1c57-def0-4e68-9b76-ea8f9d28b2ab",
   "metadata": {},
   "source": [
    "## Enable Day-Based Partitioning (Start of Partition Evolution Test)\n",
    "This step adds a day-based partitioning rule on tpep_pickup_datetime.\n",
    "It's the first step in testing partition evolution — a unique feature of Apache Iceberg that allows changing partition strategies over time without rewriting existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95ae6258-f935-477e-8a16-701275fb7f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE nyc.taxis ADD PARTITION FIELD days(tpep_pickup_datetime)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acbad3f-7820-41a9-8487-e3c545ed30c9",
   "metadata": {},
   "source": [
    "## Check Existing Partitions\n",
    "This shows how the nyc.taxis table is currently partitioned.\n",
    "\n",
    "Right now, the result shows partition = {NULL} and spec_id = 0, which means all the data was written before partitioning was added. There are no partitions yet — everything is stored together.\n",
    "\n",
    "This confirms the starting point before testing partition evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b20f21b5-4eba-4ba9-a418-127820e0da6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+------------------------+\n",
      "|partition|record_count|file_count|last_updated_snapshot_id|\n",
      "+---------+------------+----------+------------------------+\n",
      "|{NULL}   |13237765    |15        |4331219133962302771     |\n",
      "+---------+------------+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT partition, record_count, file_count, last_updated_snapshot_id  FROM nyc.taxis.partitions\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde9a74-60a1-49c8-9179-fa9b10429180",
   "metadata": {},
   "source": [
    "## Set Write Strategy for New Data\n",
    "These commands configure how new data will be written into the nyc.taxis table:\n",
    "\n",
    "WRITE DISTRIBUTED BY PARTITION tells Iceberg to group data by partition when writing files (useful after enabling partitioning).\n",
    "\n",
    "WRITE ORDERED BY tpep_pickup_datetime, VendorID sorts rows within each partition by pickup time and vendor ID.\n",
    "\n",
    "This helps optimize file layout for better query performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b2d5a93-4004-422b-8201-b876fe45c09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE nyc.taxis WRITE DISTRIBUTED BY PARTITION\")\n",
    "spark.sql(\"ALTER TABLE nyc.taxis WRITE ORDERED BY tpep_pickup_datetime, VendorID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530b8228-9361-40b7-89fe-3c7f974da508",
   "metadata": {},
   "source": [
    "## Rewrite Existing Data Files with New Partitioning\n",
    "This command rewrites the existing data in the nyc.taxis table using Iceberg’s rewrite_data_files procedure.\n",
    "\n",
    "It applies the new day-based partitioning and improves file layout by:\n",
    "\n",
    "Compacting small files (min-input-files = 1)\n",
    "\n",
    "Targeting ~128MB file sizes\n",
    "\n",
    "Allowing partial progress commits in case of failures\n",
    "\n",
    "This step brings older unpartitioned data in line with the new table configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83d8ebfa-c219-46fa-bfca-9119e3b16f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[rewritten_data_files_count: int, added_data_files_count: int, rewritten_bytes_count: bigint, failed_data_files_count: int]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "  CALL lake.system.rewrite_data_files(\n",
    "    table => 'nyc.taxis',\n",
    "    strategy => 'binpack',\n",
    "    options => map(\n",
    "      'min-input-files','1',                 -- be aggressive: rewrite even tiny groups\n",
    "      'target-file-size-bytes','134217728',  -- ~128MB target size\n",
    "      'partial-progress.enabled','true'      -- commit progress even if all tasks don't finish\n",
    "    )\n",
    "  )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb72adc-2c0f-47d5-8c64-9c88902ef06a",
   "metadata": {},
   "source": [
    "## Check Updated Partitions After Rewrite\n",
    "Running this query again now shows actual partition values instead of {NULL}.\n",
    "This confirms that the rewrite_data_files command applied the new day-based partitioning to existing data.\n",
    "\n",
    "You should now see multiple partitions, each representing a different day from tpep_pickup_datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a70d630b-b729-4034-abb2-79d417dc2926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----------+------------------------+\n",
      "|partition   |record_count|file_count|last_updated_snapshot_id|\n",
      "+------------+------------+----------+------------------------+\n",
      "|{2025-04-30}|20          |1         |8962731662446708812     |\n",
      "|{2025-05-01}|157140      |3         |8962731662446708812     |\n",
      "|{2025-05-04}|142682      |3         |8962731662446708812     |\n",
      "|{2025-05-05}|137188      |3         |8962731662446708812     |\n",
      "|{2025-05-02}|147086      |3         |8962731662446708812     |\n",
      "|{2025-05-03}|164004      |4         |8962731662446708812     |\n",
      "|{2025-05-08}|165824      |4         |8962731662446708812     |\n",
      "|{2025-05-09}|163691      |3         |8962731662446708812     |\n",
      "|{2025-05-06}|144921      |3         |8962731662446708812     |\n",
      "|{2025-05-07}|149451      |3         |8962731662446708812     |\n",
      "|{2025-05-12}|128681      |3         |8962731662446708812     |\n",
      "|{2025-05-13}|151498      |3         |8962731662446708812     |\n",
      "|{2025-05-10}|159387      |4         |8962731662446708812     |\n",
      "|{2025-05-11}|147535      |3         |8962731662446708812     |\n",
      "|{2025-05-16}|163632      |4         |8962731662446708812     |\n",
      "|{2025-05-17}|179126      |3         |8962731662446708812     |\n",
      "|{2025-05-14}|162954      |4         |8962731662446708812     |\n",
      "|{2025-05-15}|168804      |3         |8962731662446708812     |\n",
      "|{2025-05-20}|151301      |4         |8962731662446708812     |\n",
      "|{2025-05-21}|164793      |3         |8962731662446708812     |\n",
      "+------------+------------+----------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT partition, record_count, file_count, last_updated_snapshot_id  FROM nyc.taxis.partitions\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a5287-a5cb-4373-af08-62184463eaf1",
   "metadata": {},
   "source": [
    "## Verify Rewrite in Table History\n",
    "This query checks the nyc.taxis.history table to confirm that the data rewrite created a new snapshot.\n",
    "You should see a new entry at the top, showing when the rewrite was committed and linking it to a new snapshot ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6692da80-9119-46f9-80ac-716436c610ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2025-08-11 16:01:41.168|8962731662446708812|4331219133962302771|true               |\n",
      "|2025-08-11 16:01:21.92 |4331219133962302771|565187721077818959 |true               |\n",
      "|2025-08-11 16:01:16.329|565187721077818959 |8062238580457074585|true               |\n",
      "|2025-08-11 16:01:07.283|8062238580457074585|NULL               |true               |\n",
      "|2025-08-11 16:00:45.126|4274936221537877971|8062238580457074585|false              |\n",
      "|2025-08-11 16:00:43.386|8062238580457074585|NULL               |true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"-- verify it took effect\n",
    "SELECT * FROM nyc.taxis.history ORDER BY made_current_at DESC;\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fb912-b1a9-4bcc-8abb-7a921b9a1763",
   "metadata": {},
   "source": [
    "## Roll Back to Original Snapshot (Before Partitioning)\n",
    "This command reverts the nyc.taxis table back to the original snapshot (snapshot_id_1), before any partitioning or file rewrites were applied.\n",
    "\n",
    "It’s part of testing partition evolution — showing that Iceberg lets you roll back to earlier table versions, even if the partitioning strategy has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c21f78a-a998-47af-a335-b5dfd4b24bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[previous_snapshot_id: bigint, current_snapshot_id: bigint]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\" \n",
    "CALL lake.system.rollback_to_snapshot('nyc.taxis', {snapshot_id_1})\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde9ece-c1e9-441e-a774-70db2e5d67ab",
   "metadata": {},
   "source": [
    "## Confirm Partition State After Rollback\n",
    "After rolling back to snapshot_id_1, this query shows the partition state of the table has returned to {NULL}.\n",
    "This means the table is now back to its unpartitioned state — as it was before any partitioning was added or data was rewritten.\n",
    "\n",
    "It confirms that Iceberg’s rollback also restores the previous partition layout, not just the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b132cf9-4610-4c0d-b656-afcf16f37974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+------------------------+\n",
      "|partition|record_count|file_count|last_updated_snapshot_id|\n",
      "+---------+------------+----------+------------------------+\n",
      "|{NULL}   |8914805     |10        |8062238580457074585     |\n",
      "+---------+------------+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT partition, record_count, file_count, last_updated_snapshot_id  FROM nyc.taxis.partitions\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e862d-dd19-4797-a58e-8f7162a52307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
