{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ecff3b-ea3e-4565-bdfc-40e3c5f0c9f8",
   "metadata": {},
   "source": [
    "## Spark Session for Hudi on MinIO\n",
    "\n",
    "This sets up Spark to use **Apache Hudi** with **MinIO** as the storage backend.\n",
    "\n",
    "- Adds Hudi support via `hudi-spark3.5-bundle`\n",
    "- Enables Hudi SQL features\n",
    "- Connects to MinIO using S3A configs\n",
    "- Increases memory to avoid Java heap errors\n",
    "- Reduces shuffle partitions for local use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c45be33e-07f5-45a5-b3e4-b08eced9b9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "packages = \",\".join([\n",
    "    \"org.apache.hudi:hudi-spark3.5-bundle_2.12:1.0.2\",   \n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "    \"com.amazonaws:aws-java-sdk-bundle:1.12.530\",\n",
    "])\n",
    "# Create SparkSession with Hudi + S3A (MinIO) support\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"Hudi Lakehouse on MinIO\")\n",
    "      # Hudi SQL support\n",
    "      .config(\"spark.jars.packages\", packages)\n",
    "      .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "      .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "       # Mandatory serializer for Hudi\n",
    "      .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "      # S3A configuration for MinIO access\n",
    "      .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "      .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "      .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "      .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "      .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "      .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "      # Prevent Java heap errors when writing/upserting\n",
    "      .config(\"spark.driver.memory\", \"4g\") \n",
    "      .config(\"spark.executor.memory\", \"4g\")\n",
    "      # Reduce shuffle overhead for local runs\n",
    "      .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "print(\"Spark:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82f8ba-bc20-4a69-8fa4-4cf1582c657b",
   "metadata": {},
   "source": [
    "## Create Hudi Table in S3 (MinIO)\n",
    "\n",
    "This creates a Hudi table `nyc.taxis_hudi` using the **Copy-on-Write (COW)** storage type.\n",
    "\n",
    "- Partitioned by `partitionpath`\n",
    "- Uses `uuid` as the primary key\n",
    "- `ts` is the pre-combine field (used to deduplicate records)\n",
    "- Data is stored in MinIO at `s3a://my-bucket/hudi-lakehouse/nyc/taxis`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523874a4-8aee-4efa-8ed6-9e58437c3562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS nyc\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nyc.taxis_hudi (\n",
    "  VendorID INT,\n",
    "  tpep_pickup_datetime TIMESTAMP,\n",
    "  tpep_dropoff_datetime TIMESTAMP,\n",
    "  passenger_count BIGINT,\n",
    "  trip_distance DOUBLE,\n",
    "  RatecodeID BIGINT,\n",
    "  store_and_fwd_flag STRING,\n",
    "  PULocationID INT,\n",
    "  DOLocationID INT,\n",
    "  payment_type BIGINT,\n",
    "  fare_amount DOUBLE,\n",
    "  extra DOUBLE,\n",
    "  mta_tax DOUBLE,\n",
    "  tip_amount DOUBLE,\n",
    "  tolls_amount DOUBLE,\n",
    "  improvement_surcharge DOUBLE,\n",
    "  total_amount DOUBLE,\n",
    "  congestion_surcharge DOUBLE,\n",
    "  Airport_fee DOUBLE,\n",
    "  cbd_congestion_fee DOUBLE,\n",
    "  ts TIMESTAMP,\n",
    "  uuid STRING,\n",
    "  partitionpath STRING\n",
    ")\n",
    "USING hudi\n",
    "PARTITIONED BY (partitionpath)\n",
    "TBLPROPERTIES (\n",
    "  type = 'cow', -- или 'mor'\n",
    "  primaryKey = 'uuid',\n",
    "  preCombineField = 'ts'\n",
    ")\n",
    "LOCATION 's3a://my-bucket/hudi-lakehouse/nyc/taxis'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed14976-74be-4d5f-8b4b-edeb9f6d750d",
   "metadata": {},
   "source": [
    "## Insert Data into Hudi Table\n",
    "\n",
    "This inserts data into the `nyc.taxis_hudi` table from the May Parquet file, adding required fields:\n",
    "\n",
    "- `uuid`: a unique ID for each row (Hudi primary key)\n",
    "- `ts`: timestamp for preCombine logic\n",
    "- `partitionpath`: converted from `PULocationID` to use as partition column\n",
    "\n",
    "⚠️ This insert **initially failed** with `java.lang.OutOfMemoryError: Java heap space`.  \n",
    "To fix it, memory and shuffle settings were adjusted:\n",
    "\n",
    "```python\n",
    ".config(\"spark.driver.memory\", \"4g\") \n",
    ".config(\"spark.executor.memory\", \"4g\")\n",
    ".config(\"spark.sql.shuffle.partitions\", \"8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a55387-255d-4f87-b8d7-8b24a24b2351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO nyc.taxis_hudi\n",
    "SELECT\n",
    "  *,\n",
    "  uuid() AS uuid,\n",
    "  tpep_pickup_datetime AS ts,\n",
    "  CAST(PULocationID AS STRING) AS partitionpath\n",
    "FROM parquet.`s3a://my-bucket/raw-files/yellow_tripdata_2025-05.parquet`\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563c33d-1010-443b-89eb-446dce794146",
   "metadata": {},
   "source": [
    "## Preview Hudi Table with Metadata Columns\n",
    "\n",
    "This query shows a few rows from the `nyc.taxis_hudi` table, including internal Hudi metadata fields:\n",
    "\n",
    "- `_hoodie_commit_time`: when the record was written\n",
    "- `_hoodie_commit_seqno`: commit sequence number\n",
    "- `_hoodie_record_key`: the primary key (`uuid`)\n",
    "- `_hoodie_partition_path`: the partition used\n",
    "- `_hoodie_file_name`: the file storing the row\n",
    "\n",
    "These metadata fields are automatically managed by Hudi and useful for debugging and tracking data lineage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b27870-4b88-4392-a98b-e33eac0ae750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+----------------------+--------------------+--------+------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|VendorID|total_amount|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+--------+------------+\n",
      "|  20250810152622768|20250810152622768...|5219b7e4-1690-4c1...|     partitionpath=132|e89086e7-dfd5-403...|       2|       98.88|\n",
      "|  20250810152622768|20250810152622768...|488de60d-9b90-499...|     partitionpath=132|e89086e7-dfd5-403...|       2|       54.55|\n",
      "|  20250810152622768|20250810152622768...|19137f4f-c02a-40e...|     partitionpath=132|e89086e7-dfd5-403...|       2|      103.86|\n",
      "|  20250810152622768|20250810152622768...|51c60e16-661b-486...|     partitionpath=132|e89086e7-dfd5-403...|       2|       83.44|\n",
      "|  20250810152622768|20250810152622768...|d299710e-b490-4a2...|     partitionpath=132|e89086e7-dfd5-403...|       2|       36.05|\n",
      "|  20250810152622768|20250810152622768...|c9d34ebe-392d-424...|     partitionpath=132|e89086e7-dfd5-403...|       1|       80.19|\n",
      "|  20250810152622768|20250810152622768...|909ce975-b5b3-4aa...|     partitionpath=132|e89086e7-dfd5-403...|       2|       98.88|\n",
      "|  20250810152622768|20250810152622768...|7c3291d8-7312-486...|     partitionpath=132|e89086e7-dfd5-403...|       2|       73.25|\n",
      "|  20250810152622768|20250810152622768...|c189428b-cf33-450...|     partitionpath=132|e89086e7-dfd5-403...|       2|       24.75|\n",
      "|  20250810152622768|20250810152622768...|3b2ec73c-23d9-43e...|     partitionpath=132|e89086e7-dfd5-403...|       2|      -46.45|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT _hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, VendorID, total_amount FROM nyc.taxis_hudi LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ecfa1-5981-433b-9eae-2859ef6e1b6f",
   "metadata": {},
   "source": [
    "## List Hudi Partitions\n",
    "\n",
    "Lists all partition values registered for `nyc.taxis_hudi` in the catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5221857-dc64-40d6-8405-36f0ff33438c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        partition|\n",
      "+-----------------+\n",
      "|  partitionpath=1|\n",
      "| partitionpath=10|\n",
      "|partitionpath=100|\n",
      "|partitionpath=101|\n",
      "|partitionpath=102|\n",
      "|partitionpath=106|\n",
      "|partitionpath=107|\n",
      "|partitionpath=108|\n",
      "|partitionpath=109|\n",
      "| partitionpath=11|\n",
      "|partitionpath=111|\n",
      "|partitionpath=112|\n",
      "|partitionpath=113|\n",
      "|partitionpath=114|\n",
      "|partitionpath=115|\n",
      "|partitionpath=116|\n",
      "|partitionpath=117|\n",
      "|partitionpath=118|\n",
      "|partitionpath=119|\n",
      "| partitionpath=12|\n",
      "+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW PARTITIONS nyc.taxis_hudi;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8676c0ea-31bd-40da-a53c-eda8b56ee576",
   "metadata": {},
   "source": [
    "## Show Hudi Commit History\n",
    "\n",
    "This command lists all commits made to the `nyc.taxis_hudi` table.  \n",
    "Each commit represents an insert, update, or delete operation and includes:\n",
    "\n",
    "- Commit time\n",
    "- Operation type (e.g. insert, upsert)\n",
    "- Total number of records written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "207f178f-ba86-4016-a9b1-30e28e150d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------+------+-------------------+-----------------+-------------------+------------------------+---------------------+----------------------------+------------+\n",
      "|commit_time      |state_transition_time|action|total_bytes_written|total_files_added|total_files_updated|total_partitions_written|total_records_written|total_update_records_written|total_errors|\n",
      "+-----------------+---------------------+------+-------------------+-----------------+-------------------+------------------------+---------------------+----------------------------+------------+\n",
      "|20250810152622768|20250810152718110    |commit|462193941          |269              |0                  |260                     |4591845              |0                           |0           |\n",
      "+-----------------+---------------------+------+-------------------+-----------------+-------------------+------------------------+---------------------+----------------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CALL show_commits(table => 'nyc.taxis_hudi')\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f71db8-5e00-4cf9-9149-00c3e73ee5ce",
   "metadata": {},
   "source": [
    "# Simulate New Batch Ingestion into Hudi Table\n",
    "\n",
    "This insert loads a small random sample (~10%) from the June dataset into the `nyc.taxis_hudi` table.\n",
    "\n",
    "As before, it adds required fields:\n",
    "\n",
    "- `ts`: used for preCombine logic  \n",
    "- `uuid`: unique ID for the primary key  \n",
    "- `partitionpath`: determines where the data will be stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6e914c4-8cfc-450e-a845-0e0084d96d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO nyc.taxis_hudi\n",
    "SELECT\n",
    "  *,\n",
    "  tpep_pickup_datetime AS ts,\n",
    "  uuid() AS uuid,\n",
    "  CAST(PULocationID AS STRING) AS partitionpath\n",
    "FROM parquet.`s3a://my-bucket/raw-files/yellow_tripdata_2025-06.parquet` WHERE rand() < 0.10;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca52db7-f316-4a3e-9993-caa22195bc5a",
   "metadata": {},
   "source": [
    "## Verify New Commit After Insert\n",
    "\n",
    "This command shows the updated commit history of the `nyc.taxis_hudi` table.\n",
    "\n",
    "A new commit should appear, reflecting the recent insert of the June sample.  \n",
    "Each entry includes the commit timestamp, operation type, and number of records written.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5579a978-4cb9-47f2-9959-9959648939cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------+------+-------------------+-----------------+-------------------+------------------------+---------------------+----------------------------+------------+\n",
      "|commit_time      |state_transition_time|action|total_bytes_written|total_files_added|total_files_updated|total_partitions_written|total_records_written|total_update_records_written|total_errors|\n",
      "+-----------------+---------------------+------+-------------------+-----------------+-------------------+------------------------+---------------------+----------------------------+------------+\n",
      "|20250810152835682|20250810152857714    |commit|433786524          |0                |256                |256                     |4294877              |0                           |0           |\n",
      "|20250810152622768|20250810152718110    |commit|462193941          |269              |0                  |260                     |4591845              |0                           |0           |\n",
      "+-----------------+---------------------+------+-------------------+-----------------+-------------------+------------------------+---------------------+----------------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CALL show_commits(table => 'nyc.taxis_hudi')\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9e71c2-dfa1-430f-9918-09e738e8342a",
   "metadata": {},
   "source": [
    "## Time Travel in Hudi\n",
    "\n",
    "To read the Hudi table as it was at a specific point in time, set the `as.of.instant` option to a past commit time.\n",
    "\n",
    "Replace commit_time with a valid value from show_commits. This allows you to view the table state as of that commit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd2f981b-1610-4b72-a8b5-efdcdeb61553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+----------------------+--------------------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+-------------------+--------------------+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|                 ts|                uuid|partitionpath|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+-------------------+--------------------+-------------+\n",
      "|  20250810152622768|20250810152622768...|5219b7e4-1690-4c1...|     partitionpath=132|e89086e7-dfd5-403...|       2| 2025-05-11 13:34:57|  2025-05-11 14:34:26|              2|        19.48|         2|                 N|         132|         140|           1|       70.0|  0.0|    0.5|     16.19|        6.94|                  1.0|       98.88|                 2.5|       1.75|               0.0|2025-05-11 13:34:57|5219b7e4-1690-4c1...|          132|\n",
      "|  20250810152622768|20250810152622768...|488de60d-9b90-499...|     partitionpath=132|e89086e7-dfd5-403...|       2| 2025-05-05 15:51:56|  2025-05-05 16:38:43|              1|        10.97|         1|                 N|         132|         129|           2|       51.3|  0.0|    0.5|       0.0|         0.0|                  1.0|       54.55|                 0.0|       1.75|               0.0|2025-05-05 15:51:56|488de60d-9b90-499...|          132|\n",
      "|  20250810152622768|20250810152622768...|19137f4f-c02a-40e...|     partitionpath=132|e89086e7-dfd5-403...|       2| 2025-05-21 08:11:01|  2025-05-21 09:24:43|              2|        16.98|         2|                 N|         132|         230|           1|       70.0|  0.0|    0.5|     20.42|        6.94|                  1.0|      103.86|                 2.5|       1.75|              0.75|2025-05-21 08:11:01|19137f4f-c02a-40e...|          132|\n",
      "|  20250810152622768|20250810152622768...|51c60e16-661b-486...|     partitionpath=132|e89086e7-dfd5-403...|       2| 2025-05-11 19:37:33|  2025-05-11 20:55:12|              4|        18.12|         2|                 N|         132|          48|           2|       70.0|  0.0|    0.5|       0.0|        6.94|                  1.0|       83.44|                 2.5|       1.75|              0.75|2025-05-11 19:37:33|51c60e16-661b-486...|          132|\n",
      "|  20250810152622768|20250810152622768...|d299710e-b490-4a2...|     partitionpath=132|e89086e7-dfd5-403...|       2| 2025-05-08 16:08:34|  2025-05-08 16:32:10|              1|         6.02|         1|                 N|         132|         203|           2|       30.3|  2.5|    0.5|       0.0|         0.0|                  1.0|       36.05|                 0.0|       1.75|               0.0|2025-05-08 16:08:34|d299710e-b490-4a2...|          132|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+-------------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "commit_time = \"20250810152622768\"   \n",
    "df = spark.read.format(\"hudi\") \\\n",
    "    .option(\"as.of.instant\", commit_time) \\\n",
    "    .load(\"s3a://my-bucket/hudi-lakehouse/nyc/taxis/\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c00d1-aeae-40f6-86c4-8931a3b84ab9",
   "metadata": {},
   "source": [
    "## Add and Populate New Column in Hudi Table\n",
    "\n",
    "A new column `fare_bucket` is added to the `nyc.taxis_hudi` table to categorize trips by fare amount.\n",
    "\n",
    "Steps:\n",
    "1. `ADD COLUMNS` adds the new column.\n",
    "2. `REFRESH TABLE` ensures Spark sees the updated schema.\n",
    "3. `UPDATE` fills `fare_bucket` with values:\n",
    "   - `'low'` for fares under 10  \n",
    "   - `'mid'` for fares between 10 and 30  \n",
    "   - `'high'` for fares 30 and above\n",
    "\n",
    "Finally, the table schema and value distribution are checked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1b21f0b-83ab-499b-9f08-ee50fa8b26ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------+-------+\n",
      "|col_name              |data_type|comment|\n",
      "+----------------------+---------+-------+\n",
      "|_hoodie_commit_time   |string   |NULL   |\n",
      "|_hoodie_commit_seqno  |string   |NULL   |\n",
      "|_hoodie_record_key    |string   |NULL   |\n",
      "|_hoodie_partition_path|string   |NULL   |\n",
      "|_hoodie_file_name     |string   |NULL   |\n",
      "|VendorID              |int      |NULL   |\n",
      "|tpep_pickup_datetime  |timestamp|NULL   |\n",
      "|tpep_dropoff_datetime |timestamp|NULL   |\n",
      "|passenger_count       |bigint   |NULL   |\n",
      "|trip_distance         |double   |NULL   |\n",
      "|RatecodeID            |bigint   |NULL   |\n",
      "|store_and_fwd_flag    |string   |NULL   |\n",
      "|PULocationID          |int      |NULL   |\n",
      "|DOLocationID          |int      |NULL   |\n",
      "|payment_type          |bigint   |NULL   |\n",
      "|fare_amount           |double   |NULL   |\n",
      "|extra                 |double   |NULL   |\n",
      "|mta_tax               |double   |NULL   |\n",
      "|tip_amount            |double   |NULL   |\n",
      "|tolls_amount          |double   |NULL   |\n",
      "+----------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+--------+\n",
      "|fare_bucket|count(1)|\n",
      "+-----------+--------+\n",
      "|        low|  466592|\n",
      "|       high| 1341306|\n",
      "|        mid| 3216789|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE nyc.taxis_hudi ADD COLUMNS (fare_bucket STRING)\")\n",
    "\n",
    "spark.sql(\"REFRESH TABLE nyc.taxis_hudi\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "UPDATE nyc.taxis_hudi\n",
    "SET fare_bucket = CASE\n",
    "  WHEN total_amount < 10 THEN 'low'\n",
    "  WHEN total_amount < 30 THEN 'mid'\n",
    "  ELSE 'high'\n",
    "END\n",
    "WHERE total_amount IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DESCRIBE TABLE nyc.taxis_hudi\").show(truncate=False)\n",
    "spark.sql(\"SELECT fare_bucket, COUNT(*) FROM nyc.taxis_hudi GROUP BY fare_bucket\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86cb82-271c-4999-a84a-9695e29a80e8",
   "metadata": {},
   "source": [
    "### Attempt to Drop a Column from a Hudi Table\n",
    "\n",
    "I tried to remove the `fare_bucket` column from the Hudi table using:\n",
    "\n",
    "```python\n",
    "spark.sql(\"ALTER TABLE nyc.taxis_hudi DROP COLUMNS (fare_bucket);\")\n",
    "```\n",
    "\n",
    "However, this failed with:\n",
    "\n",
    "AnalysisException: [UNSUPPORTED_FEATURE.TABLE_OPERATION] \n",
    "The feature is not supported: Table `spark_catalog`.`nyc`.`taxis_hudi` does not support DROP COLUMN. \n",
    "Please check the current catalog and namespace to make sure the qualified table name is expected, \n",
    "and also check the catalog implementation which is configured by \"spark.sql.catalog\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf91781-2aba-4d69-8514-5afa340059f2",
   "metadata": {},
   "source": [
    "## Adding a New Column to a Hudi Table on the Fly (Writer-Side Schema Evolution)\n",
    "\n",
    "Hudi supports **schema evolution** when writing data — meaning you can add new columns directly in your writer DataFrame, and Hudi will update the table schema automatically.  \n",
    "\n",
    "The following PySpark example adds a new nullable column `tip_rate_pct` (calculated as a percentage), generates a UUID record key, ensures Hudi-required fields (`recordkey`, `precombine`, and `partitionpath`), and also includes a `fare_bucket` column set to `NULL`\n",
    "\n",
    "Important:\n",
    "\n",
    "This approach only works during write (schema evolution in writer).\n",
    "If you want to add a column using SQL, you must use ALTER TABLE ... ADD COLUMNS (...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de6ccf7b-0295-4df9-9b31-59ea4283b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pyspark.sql.functions import col, when, lit, udf, round as sround\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# UDF to generate UUID strings\n",
    "@udf(returnType=StringType())\n",
    "def gen_uuid():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# Load original data\n",
    "df = spark.read.parquet(\"s3a://my-bucket/raw-files/yellow_tripdata_2025-06.parquet\")\n",
    "\n",
    "# Add a NEW nullable column: tip_rate_pct (percent), plus Hudi-required fields\n",
    "df_with_new = (\n",
    "    df.withColumn(\n",
    "        \"tip_rate_pct\",\n",
    "        when(col(\"total_amount\").isNull(), lit(None).cast(\"double\"))   # ensure nullable in writer schema\n",
    "        .when(col(\"total_amount\") <= 0, lit(None).cast(\"double\"))\n",
    "        .otherwise(sround(col(\"tip_amount\") / col(\"total_amount\") * 100.0, 2))\n",
    "    )\n",
    "    .withColumn(\"ts\", col(\"tpep_pickup_datetime\"))\n",
    "    .withColumn(\"uuid\", gen_uuid())\n",
    "    .withColumn(\"partitionpath\", col(\"PULocationID\").cast(\"string\"))\n",
    ")\n",
    "\n",
    "# Cast NTZ -> timestamp for Hudi/Avro compatibility\n",
    "df_fixed = (\n",
    "    df_with_new\n",
    "      .withColumn(\"ts\", col(\"ts\").cast(\"timestamp\"))\n",
    "      .withColumn(\"tpep_pickup_datetime\", col(\"tpep_pickup_datetime\").cast(\"timestamp\"))\n",
    "      .withColumn(\"tpep_dropoff_datetime\", col(\"tpep_dropoff_datetime\").cast(\"timestamp\"))\n",
    "      .withColumn(\"fare_bucket\", lit(None).cast(\"string\"))  # match table schema\n",
    ")\n",
    "\n",
    "# Write with schema evolution on (adds tip_rate_pct automatically)\n",
    "(df_fixed.write.format(\"hudi\")\n",
    "  .option(\"hoodie.datasource.write.operation\", \"upsert\")\n",
    "  .option(\"hoodie.datasource.write.recordkey.field\", \"uuid\")\n",
    "  .option(\"hoodie.datasource.write.precombine.field\", \"ts\")\n",
    "  .option(\"hoodie.datasource.write.partitionpath.field\", \"partitionpath\")\n",
    "  .option(\"hoodie.schema.on.write.enable\", \"true\")\n",
    "  .option(\"hoodie.datasource.write.reconcile.schema\", \"true\")\n",
    "  .option(\"hoodie.write.set.null.for.missing.columns\", \"true\")\n",
    "  .option(\"hoodie.datasource.write.hive_style_partitioning\", \"true\")\n",
    "  .mode(\"append\")\n",
    "  .save(\"s3a://my-bucket/hudi-lakehouse/nyc/taxis\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39682670-eb08-4693-9915-14fe48655b84",
   "metadata": {},
   "source": [
    "## Inspecting a Hudi Table via Snapshot Read\n",
    "\n",
    "To explore the current schema and sample data in a Hudi table, you can perform a **snapshot read** by loading the table path directly into a DataFrame.  \n",
    "Once loaded, you register it as a temporary view and then use standard SQL commands to:\n",
    "\n",
    "1. **Describe the table** — view all columns, their data types, and nullability.\n",
    "2. **Select specific columns** — retrieve `fare_bucket`, `tip_rate_pct`, and `partitionpath` to confirm that recently added fields are present and populated (or `NULL` for older records).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4b8ca95-46d5-4e21-a48d-ca7e5b2805d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------+-------+\n",
      "|col_name              |data_type|comment|\n",
      "+----------------------+---------+-------+\n",
      "|_hoodie_commit_time   |string   |NULL   |\n",
      "|_hoodie_commit_seqno  |string   |NULL   |\n",
      "|_hoodie_record_key    |string   |NULL   |\n",
      "|_hoodie_partition_path|string   |NULL   |\n",
      "|_hoodie_file_name     |string   |NULL   |\n",
      "|VendorID              |int      |NULL   |\n",
      "|tpep_pickup_datetime  |timestamp|NULL   |\n",
      "|tpep_dropoff_datetime |timestamp|NULL   |\n",
      "|passenger_count       |bigint   |NULL   |\n",
      "|trip_distance         |double   |NULL   |\n",
      "|RatecodeID            |bigint   |NULL   |\n",
      "|store_and_fwd_flag    |string   |NULL   |\n",
      "|PULocationID          |int      |NULL   |\n",
      "|DOLocationID          |int      |NULL   |\n",
      "|payment_type          |bigint   |NULL   |\n",
      "|fare_amount           |double   |NULL   |\n",
      "|extra                 |double   |NULL   |\n",
      "|mta_tax               |double   |NULL   |\n",
      "|tip_amount            |double   |NULL   |\n",
      "|tolls_amount          |double   |NULL   |\n",
      "+----------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+------------+-------------+\n",
      "|fare_bucket|tip_rate_pct|partitionpath|\n",
      "+-----------+------------+-------------+\n",
      "|       high|        NULL|          132|\n",
      "|       high|        NULL|          132|\n",
      "|       high|        NULL|          132|\n",
      "|       high|        NULL|          132|\n",
      "|       high|        NULL|          132|\n",
      "|       high|        NULL|          132|\n",
      "|       high|        NULL|          132|\n",
      "|       high|        NULL|          132|\n",
      "|        mid|        NULL|          132|\n",
      "|        low|        NULL|          132|\n",
      "+-----------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_path = spark.read.format(\"hudi\").load(\"s3a://my-bucket/hudi-lakehouse/nyc/taxis\")\n",
    "df_path.createOrReplaceTempView(\"hudi_taxis_snapshot\")\n",
    "\n",
    "spark.sql(\"DESCRIBE TABLE hudi_taxis_snapshot\").show(truncate=False)\n",
    "spark.sql(\"SELECT fare_bucket, tip_rate_pct, partitionpath FROM hudi_taxis_snapshot LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3364cd-33a8-494a-a9ca-aec4ae9283c1",
   "metadata": {},
   "source": [
    "## Partition Evolution in Hudi\n",
    "\n",
    "Apache Hudi **does not support partition evolution**.\n",
    "\n",
    "Once a table is created with a specific partition column (e.g. `partitionpath`), it cannot be changed or updated later.  \n",
    "Changing the partitioning requires creating a new table with the desired partition spec and rewriting the data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
