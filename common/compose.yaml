name: lakehouse-local-setup

networks:
  infra_network:
    driver: bridge

services:
  minio:
    image: minio/minio:RELEASE.2025-07-23T15-54-02Z
    container_name: minio
    ports:
      - "9000:9000"   # S3 API
      - "9001:9001"   # Console UI
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - ./data:/data
    command: server --console-address ":9001" /data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 20s
      timeout: 20s
      retries: 3
    networks:
      - infra_network

  aws:
    image: amazon/aws-cli
    container_name: aws-cli
    entrypoint: ["/bin/bash", "-c"]
    volumes:
      - ./raw_data:/seed:ro
    command: >
      "sleep 5 &&
       aws --endpoint-url http://minio:9000 s3 mb s3://my-bucket --region eu-west-1 || true
       aws --endpoint-url http://minio:9000 s3 sync /seed s3://my-bucket/raw-files/"
    environment:
      AWS_ACCESS_KEY_ID: "minioadmin"
      AWS_SECRET_ACCESS_KEY: "minioadmin"
      AWS_DEFAULT_REGION: "eu-west-1"
    depends_on:
      minio:
        condition: service_healthy
    networks:
      - infra_network


  jupyter:
    image: quay.io/jupyter/pyspark-notebook:spark-3.5.3
    container_name: jupyter
    ports:
      - "8888:8888"    
      - "4040:4040"   
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - PYSPARK_SUBMIT_ARGS=pyspark-shell 
      #--packages org.apache.hadoop:hadoop-aws:3.3.4,software.amazon.awssdk:bundle:2.32.16 --conf spark.hadoop.fs.s3a.threads.keepalivetime=60 pyspark-shell
    command: start-notebook.sh --NotebookApp.token='' --NotebookApp.allow_origin='*'
    volumes:
      - ./notebooks/work:/home/jovyan/work
      - ./notebooks/sparkdata:/sparkdata
    depends_on:
      minio:
        condition: service_healthy
      aws:
        condition: service_completed_successfully
    networks:
      - infra_network

